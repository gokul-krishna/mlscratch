{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Gokul Krishna Guruswamy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pure Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T20:25:40.791438Z",
     "start_time": "2019-05-04T20:25:40.788875Z"
    }
   },
   "outputs": [],
   "source": [
    "from warnings import filterwarnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T20:25:40.802864Z",
     "start_time": "2019-05-04T20:25:40.792765Z"
    }
   },
   "outputs": [],
   "source": [
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T20:25:40.814007Z",
     "start_time": "2019-05-04T20:25:40.804164Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from decimal import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T20:25:40.825273Z",
     "start_time": "2019-05-04T20:25:40.815351Z"
    }
   },
   "outputs": [],
   "source": [
    "def vec_sigmoid(z):\n",
    "    # python precision safe sigmoid function\n",
    "    return [(1.0 / (1.0 + float(Decimal(-i).exp()))) for i in z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T20:25:40.837640Z",
     "start_time": "2019-05-04T20:25:40.826475Z"
    }
   },
   "outputs": [],
   "source": [
    "def mean(x):\n",
    "    return sum(x) / len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the logistic loss function as it is well suited for logistic regression because itâ€™s convex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T20:25:40.854625Z",
     "start_time": "2019-05-04T20:25:40.838921Z"
    }
   },
   "outputs": [],
   "source": [
    "def vec_logistic_loss(y, y_hat):\n",
    "    p1 = list(map(lambda x: x[0] * math.log(x[1]),  zip(y, y_hat)))\n",
    "    p2 = list(map(lambda x: (1.0 - x[0]) * math.log(1.0 - x[1]),  zip(y, y_hat)))\n",
    "    p3 = list(map(lambda x: x[0] + x[1], zip(p1, p2)))\n",
    "    return -mean(p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T20:25:40.866294Z",
     "start_time": "2019-05-04T20:25:40.856212Z"
    }
   },
   "outputs": [],
   "source": [
    "def matmul(X, Y):\n",
    "    # matrix multiplication implementation\n",
    "\n",
    "    xw, xh = len(X[0]), len(X)\n",
    "    yw, yh = len(Y[0]), len(Y)\n",
    "\n",
    "    if xw == yh:\n",
    "        \n",
    "        res = [[0 for i in range(yw)] for j in range(xh)]\n",
    "        for i in range(len(X)):\n",
    "            for j in range(len(Y[0])):\n",
    "                for k in range(len(Y)):\n",
    "                    res[i][j] += X[i][k] * Y[k][j]\n",
    "\n",
    "        return(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T20:25:40.878910Z",
     "start_time": "2019-05-04T20:25:40.868923Z"
    }
   },
   "outputs": [],
   "source": [
    "def transpose(x):\n",
    "    # matrix transpose\n",
    "    return [[x[j][i] for j in range(len(x))] for i in range(len(x[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T20:25:40.896533Z",
     "start_time": "2019-05-04T20:25:40.880446Z"
    }
   },
   "outputs": [],
   "source": [
    "def list_flatten(x):\n",
    "    return [item for sublist in x for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T20:25:40.912787Z",
     "start_time": "2019-05-04T20:25:40.898156Z"
    }
   },
   "outputs": [],
   "source": [
    "def elem_sub(X, C):\n",
    "    # element wise subtraction for matrix\n",
    "    return [[item - c for item in sublist] for c, sublist in zip(C, X)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T20:25:40.927467Z",
     "start_time": "2019-05-04T20:25:40.914368Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_data(fname):\n",
    "    # helper function to load data in to an array\n",
    "    X = []\n",
    "    y = []\n",
    "    with open(fname, 'r') as f:\n",
    "        while True:\n",
    "            buf = f.readline()\n",
    "            if not buf:\n",
    "                break\n",
    "            d = buf.split(',')\n",
    "            y.append(int(d[-1]))\n",
    "            X.append([float(i) for i in d[:-1]])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T20:25:40.959287Z",
     "start_time": "2019-05-04T20:25:40.947025Z"
    }
   },
   "outputs": [],
   "source": [
    "def fit_logistic_reg(X, y, learning_rate=0.0001, no_epochs=3000):\n",
    "\n",
    "    m = len(y)\n",
    "    # initialize weights and bias\n",
    "    W = [[0.00001] for i in range(len(X[0]))]\n",
    "    b = 0.1\n",
    "\n",
    "    for epoch in range(no_epochs):\n",
    "\n",
    "        # multiplying with weights\n",
    "        Z = matmul(X, W)\n",
    "        # adding bias terms\n",
    "        Z = [[i[0] + b] for i in Z]\n",
    "        # get log odds\n",
    "        y_hat = list(map(lambda x: vec_sigmoid(x), Z))\n",
    "        \n",
    "        # get logistic loss\n",
    "        loss = vec_logistic_loss(y, list_flatten(y_hat))\n",
    "        \n",
    "        # calculate difference between y and log odds\n",
    "        dz = list(map(lambda x: [x[1] - x[0]],  zip(y, list_flatten(y_hat))))\n",
    "        t = matmul(transpose(X), dz)\n",
    "\n",
    "        # calculating gradients for weights and bias\n",
    "        dw = list(map(lambda x: [(1.0 / m) * x[0]], t))\n",
    "        db = sum(list_flatten(dz))\n",
    "        \n",
    "        # do gradient descent and update weights & bias\n",
    "        W = elem_sub(W, list(map(lambda x: x[0] * learning_rate, dw)))\n",
    "        b = b - learning_rate * db\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"loss after {epoch} epoch is: {loss}\")\n",
    "\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T20:25:40.972390Z",
     "start_time": "2019-05-04T20:25:40.960757Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(X, W, b, thres=0.5):\n",
    "\n",
    "    # given set of observations, slope and intercept,\n",
    "    # return hard and soft predictions\n",
    "\n",
    "    Z = matmul(X, W)\n",
    "    Z = [[i[0] + b] for i in Z]\n",
    "    y_hat = list(map(lambda x: vec_sigmoid(x), Z))\n",
    "\n",
    "    return list(map(lambda x: 1 if x > thres else 0,\n",
    "                    list_flatten(y_hat))), y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T20:25:40.990031Z",
     "start_time": "2019-05-04T20:25:40.974324Z"
    }
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T20:25:40.944990Z",
     "start_time": "2019-05-04T20:25:40.928658Z"
    }
   },
   "outputs": [],
   "source": [
    "X, y = get_data('pima-indians-diabetes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T20:25:41.010104Z",
     "start_time": "2019-05-04T20:25:40.991531Z"
    }
   },
   "outputs": [],
   "source": [
    "rnd_idx = list(range(len(y)))\n",
    "random.shuffle(rnd_idx)\n",
    "split = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T20:25:41.023046Z",
     "start_time": "2019-05-04T20:25:41.014193Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = [X[i] for i in rnd_idx[:int(split * len(y))]]\n",
    "y_train = [y[i] for i in rnd_idx[:int(split * len(y))]]\n",
    "X_test = [X[i] for i in rnd_idx[int(split * len(y)):]]\n",
    "y_test = [y[i] for i in rnd_idx[int(split * len(y)):]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T20:26:36.788973Z",
     "start_time": "2019-05-04T20:25:41.025881Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after 0 epoch is: 0.7105021272591197\n",
      "loss after 100 epoch is: 0.6252818961215323\n",
      "loss after 200 epoch is: 0.610526359858133\n",
      "loss after 300 epoch is: 0.6008106540497504\n",
      "loss after 400 epoch is: 0.592628726436815\n",
      "loss after 500 epoch is: 0.5853021880187478\n",
      "loss after 600 epoch is: 0.5786227198138221\n",
      "loss after 700 epoch is: 0.5724875444809688\n",
      "loss after 800 epoch is: 0.5668279111943508\n",
      "loss after 900 epoch is: 0.5615908857469761\n",
      "loss after 1000 epoch is: 0.5567330529730734\n",
      "loss after 1100 epoch is: 0.5522176360985891\n",
      "loss after 1200 epoch is: 0.5480128862142688\n",
      "loss after 1300 epoch is: 0.544091053966279\n",
      "loss after 1400 epoch is: 0.54042767160324\n",
      "loss after 1500 epoch is: 0.537001020112731\n",
      "loss after 1600 epoch is: 0.533791716801823\n",
      "loss after 1700 epoch is: 0.530782386679928\n",
      "loss after 1800 epoch is: 0.5279573951050773\n",
      "loss after 1900 epoch is: 0.5253026268419346\n",
      "loss after 2000 epoch is: 0.5228053012003111\n",
      "loss after 2100 epoch is: 0.5204538157709458\n",
      "loss after 2200 epoch is: 0.5182376131757788\n",
      "loss after 2300 epoch is: 0.5161470665758663\n",
      "loss after 2400 epoch is: 0.514173380635905\n",
      "loss after 2500 epoch is: 0.51230850535011\n",
      "loss after 2600 epoch is: 0.5105450606645996\n",
      "loss after 2700 epoch is: 0.5088762702357914\n",
      "loss after 2800 epoch is: 0.5072959029760413\n",
      "loss after 2900 epoch is: 0.5057982212807773\n",
      "loss after 3000 epoch is: 0.504377935022495\n",
      "loss after 3100 epoch is: 0.5030301605488741\n",
      "loss after 3200 epoch is: 0.5017503840439503\n",
      "loss after 3300 epoch is: 0.5005344287097165\n",
      "loss after 3400 epoch is: 0.49937842530582627\n",
      "loss after 3500 epoch is: 0.49827878565112116\n",
      "loss after 3600 epoch is: 0.49723217874546166\n",
      "loss after 3700 epoch is: 0.4962355092161327\n",
      "loss after 3800 epoch is: 0.4952858978315696\n",
      "loss after 3900 epoch is: 0.4943806638577881\n",
      "loss after 4000 epoch is: 0.49351730906067093\n",
      "loss after 4100 epoch is: 0.49269350318107047\n",
      "loss after 4200 epoch is: 0.4919070707301966\n",
      "loss after 4300 epoch is: 0.4911559789704549\n",
      "loss after 4400 epoch is: 0.4904383269623105\n",
      "loss after 4500 epoch is: 0.4897523355711522\n",
      "loss after 4600 epoch is: 0.489096338339847\n",
      "loss after 4700 epoch is: 0.4884687731429626\n",
      "loss after 4800 epoch is: 0.4878681745476611\n",
      "loss after 4900 epoch is: 0.4872931668142242\n"
     ]
    }
   ],
   "source": [
    "W, b = fit_logistic_reg(X_train, y_train, no_epochs=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we will use metrics from sklearn, to compare test and train performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T20:26:37.578571Z",
     "start_time": "2019-05-04T20:26:36.790501Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T20:26:37.585040Z",
     "start_time": "2019-05-04T20:26:37.580922Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_metrics(X, y):\n",
    "    preds, y_hat = predict(X, W, b)\n",
    "    acc = accuracy_score(y, preds)\n",
    "    auc = roc_auc_score(y, y_hat)\n",
    "    print(classification_report(y, preds))\n",
    "    print(f\"Accuracy: {auc}, AUC: {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T20:26:37.600159Z",
     "start_time": "2019-05-04T20:26:37.587632Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class ratio: 0.3489583333333333\n"
     ]
    }
   ],
   "source": [
    "print(f\"class ratio: {sum(y) / len(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see that classes are not balanced, hence we will use auc and f1 scores to check the model performance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T20:26:37.646666Z",
     "start_time": "2019-05-04T20:26:37.603158Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.90      0.84       403\n",
      "           1       0.73      0.52      0.61       211\n",
      "\n",
      "   micro avg       0.77      0.77      0.77       614\n",
      "   macro avg       0.76      0.71      0.72       614\n",
      "weighted avg       0.77      0.77      0.76       614\n",
      "\n",
      "Accuracy: 0.8292074841532111, AUC: 0.8292074841532111\n"
     ]
    }
   ],
   "source": [
    "get_metrics(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T20:26:37.664191Z",
     "start_time": "2019-05-04T20:26:37.649115Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.86      0.79        97\n",
      "           1       0.67      0.49      0.57        57\n",
      "\n",
      "   micro avg       0.72      0.72      0.72       154\n",
      "   macro avg       0.70      0.67      0.68       154\n",
      "weighted avg       0.71      0.72      0.71       154\n",
      "\n",
      "Accuracy: 0.8191354675348164, AUC: 0.8191354675348164\n"
     ]
    }
   ],
   "source": [
    "get_metrics(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see that the test metrics are on the same level in terms of accuracy, AUC, f1-score etc. I have discussed below breifly how to avoid overfitting.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T20:26:37.713309Z",
     "start_time": "2019-05-04T20:26:37.666457Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T20:26:37.718176Z",
     "start_time": "2019-05-04T20:26:37.715705Z"
    }
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T20:26:37.752511Z",
     "start_time": "2019-05-04T20:26:37.720688Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=3000, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T20:26:37.759122Z",
     "start_time": "2019-05-04T20:26:37.754810Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_metrics_sk(X, y):\n",
    "    preds = model.predict(X)\n",
    "    y_hat = model.predict_proba(X)\n",
    "    acc = accuracy_score(y, preds)\n",
    "    auc = roc_auc_score(y, [i[1] for i in y_hat])\n",
    "    print(classification_report(y, preds))\n",
    "    print(f\"Accuracy: {acc}, AUC: {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T20:26:37.782321Z",
     "start_time": "2019-05-04T20:26:37.761548Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.91      0.85       403\n",
      "           1       0.76      0.55      0.64       211\n",
      "\n",
      "   micro avg       0.79      0.79      0.79       614\n",
      "   macro avg       0.78      0.73      0.74       614\n",
      "weighted avg       0.78      0.79      0.77       614\n",
      "\n",
      "Accuracy: 0.7850162866449512, AUC: 0.8335352157397717\n"
     ]
    }
   ],
   "source": [
    "get_metrics_sk(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T20:26:37.797852Z",
     "start_time": "2019-05-04T20:26:37.786337Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.86      0.79        97\n",
      "           1       0.66      0.47      0.55        57\n",
      "\n",
      "   micro avg       0.71      0.71      0.71       154\n",
      "   macro avg       0.70      0.66      0.67       154\n",
      "weighted avg       0.71      0.71      0.70       154\n",
      "\n",
      "Accuracy: 0.7142857142857143, AUC: 0.8260083197684934\n"
     ]
    }
   ],
   "source": [
    "get_metrics_sk(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The sklearn model with default settings seem to be performing poorly when compared with the pure python implementation, as indicated by the values of the above metrics, we could do grid search for hyperparameters like regularization, class_weight, penely etc, to achive the best model possible**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can handle overfitting in 2 ways:\n",
    "\n",
    "1. *Early Stopping*: While training the model, we'll monitor the values of some loss metric, by checking it every few iterations. Comparing these values for the validation and training data set will let us know when the model has started overfitting. That will be our cue to stop training.\n",
    "\n",
    "2. *Regularization*: We can also combat overfitting by using regularization. In this technique, we add an extra term to the cost function, which ends up penalising certain parameter configurations more than others. Examples of this are L-1 regularization and L-2 regularization.\n",
    "\n",
    "In general, the cost function under L-p regularization:\n",
    "\n",
    "$$E(\\theta,D) = L(\\theta,D) + \\lambda R(\\theta)$$\n",
    "\n",
    "where,\n",
    "- $R(\\theta) = \\lambda (\\sum_{j}|\\theta_j|^p)$, and\n",
    "- $\\lambda$ = Parameter that controls how much we regularize"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
